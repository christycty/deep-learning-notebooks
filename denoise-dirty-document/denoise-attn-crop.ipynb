{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"230513 23:30\nsubmission(14).csv -> 0.02683\ncropping, 10 epoch\n\n```\nautoencoder = keras.Sequential([\n    layers.Input(shape=img_dim),\n    # encoder\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    \n    layers.Conv2D(256, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    \n    layers.Conv2D(128, (3, 3), activation='relu'),\n    \n    layers.Dropout(0.5),\n\n    # decoder\n    layers.Conv2DTranspose(128, (3, 3), activation='relu'),\n    \n    layers.Conv2DTranspose(256, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    \n    layers.Conv2DTranspose(64, (3, 3), activation='relu'),\n    layers.Conv2D(1, (1, 1), activation='sigmoid')\n])\n```","metadata":{}},{"cell_type":"code","source":"import zipfile\nimport os\nfrom collections import Counter\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport math\n\nimport keras\nfrom keras.layers import *\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-14T12:52:50.391835Z","iopub.execute_input":"2023-05-14T12:52:50.392378Z","iopub.status.idle":"2023-05-14T12:52:50.399682Z","shell.execute_reply.started":"2023-05-14T12:52:50.392342Z","shell.execute_reply":"2023-05-14T12:52:50.398167Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"markdown","source":"### Unzip images into workspace directories","metadata":{}},{"cell_type":"code","source":"path_zip = '/kaggle/input/denoising-dirty-documents/'\npath = '/kaggle/working/'\n\nwith zipfile.ZipFile(path_zip + 'train.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)\n\nwith zipfile.ZipFile(path_zip + 'test.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)  \n    \nwith zipfile.ZipFile(path_zip + 'train_cleaned.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)  \n    \n# list of image names\ntrain_img = sorted(os.listdir(path + '/train'))\ntrain_cleaned_img = sorted(os.listdir(path + '/train_cleaned'))\ntest_img = sorted(os.listdir(path + '/test'))","metadata":{"execution":{"iopub.status.busy":"2023-05-14T12:52:50.404406Z","iopub.execute_input":"2023-05-14T12:52:50.404710Z","iopub.status.idle":"2023-05-14T12:52:50.600193Z","shell.execute_reply.started":"2023-05-14T12:52:50.404666Z","shell.execute_reply":"2023-05-14T12:52:50.599259Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"crop to 140 (height, 420/3) x 135(width,540/4)","metadata":{}},{"cell_type":"markdown","source":"## Preprocess Image","metadata":{}},{"cell_type":"code","source":"img_dim = (140, 135, 1)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T12:52:50.602049Z","iopub.execute_input":"2023-05-14T12:52:50.602863Z","iopub.status.idle":"2023-05-14T12:52:50.607899Z","shell.execute_reply.started":"2023-05-14T12:52:50.602828Z","shell.execute_reply":"2023-05-14T12:52:50.606585Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(img):\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img_np = np.asarray(img, dtype=\"float32\") \n    img_np /= 255.0\n    img_np = np.reshape(img_np, img_dim) # expand to 3d\n    return img_np","metadata":{"execution":{"iopub.status.busy":"2023-05-14T12:52:50.609325Z","iopub.execute_input":"2023-05-14T12:52:50.610031Z","iopub.status.idle":"2023-05-14T12:52:50.619463Z","shell.execute_reply.started":"2023-05-14T12:52:50.609948Z","shell.execute_reply":"2023-05-14T12:52:50.618447Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"def crop_image(img):\n    img_h, img_w = img.shape[0], img.shape[1]\n    \n    # center image by adding white paddings\n    UD_border = img_dim[0] * ((img_h - 1) // img_dim[0] + 1) - img_h\n    U, D = UD_border // 2, math.ceil(UD_border / 2)\n    \n    LR_border = img_dim[1] * ((img_w - 1) // img_dim[1] + 1) - img_w\n    L, R = LR_border // 2, math.ceil(LR_border / 2)\n    \n    padded_img = cv2.copyMakeBorder(img, U, D, L, R, cv2.BORDER_CONSTANT, value=(255, 255, 255))\n    \n    # crop image\n    img_list = []\n    no_row, no_col = (img_h + UD_border) // img_dim[0], (img_w + LR_border) // img_dim[1]\n    for row in range(no_row):\n        for col in range(no_col):\n            r1, r2 = row * img_dim[0], (row + 1) * img_dim[0]\n            c1, c2 = col * img_dim[1], (col + 1) * img_dim[1]\n            crop_img = padded_img[r1:r2, c1:c2]\n            img_list.append(crop_img)\n    \n    return img_list","metadata":{"execution":{"iopub.status.busy":"2023-05-14T12:52:50.622210Z","iopub.execute_input":"2023-05-14T12:52:50.622668Z","iopub.status.idle":"2023-05-14T12:52:50.634394Z","shell.execute_reply.started":"2023-05-14T12:52:50.622637Z","shell.execute_reply":"2023-05-14T12:52:50.633294Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"def load_img(path):\n    # load image\n    raw_img = cv2.imread(path)\n    \n    # crop image\n    img_list = crop_image(raw_img)\n    \n    # preprocess - to gray, normalize, to 3d\n    processed_list = []\n    for img in img_list:\n        processed_img = preprocess_image(img)\n        processed_list.append(processed_img)\n        \n    return processed_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_imgs(img_list):\n    n = len(img_list)\n    for i in range(n):\n        plt.subplot(math.ceil(n/4),4,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.imshow(img_list[i], cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess images\ntrain = []\ntrain_cleaned = []\ntest = []\n\nfor f in sorted(os.listdir(path + 'train/')):\n    processed_img = load_img(path + 'train/' + f)\n    for img in processed_img:\n        train.append(img)\n\nfor f in sorted(os.listdir(path + 'train_cleaned/')):\n    processed_img = load_img(path + 'train_cleaned/' + f)\n    for img in processed_img:\n        train_cleaned.append(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_imgs(train[4:8])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_imgs(train_cleaned[4:8])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train test split\nThe data size is small, so use a smaller portion of validation set","metadata":{}},{"cell_type":"code","source":"X_train = np.asarray(train)\ny_train = np.asarray(train_cleaned)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Autoencoder","metadata":{}},{"cell_type":"code","source":"inputs = Input(shape=img_dim, name='input')\n\nencoder = keras.Sequential([\n    Input(shape=img_dim, name='input'),\n    Conv2D(64, (5, 5), activation='relu'),\n    BatchNormalization(),\n    Dropout(0.5),\n    \n    Conv2D(64, (5, 5), activation='relu'),\n    MaxPooling2D((3, 3)),\n    BatchNormalization(),\n    Dropout(0.5),\n    \n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    BatchNormalization(),\n    Dropout(0.5),\n], name='encoder')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded = encoder(inputs)\nx = MultiHeadAttention(num_heads=4, key_dim=256)(encoded, encoded)\nx = add([x, encoded])\n\ndecoder = keras.Sequential([\n    UpSampling2D((2, 2)),\n    Conv2DTranspose(64, (3, 3), activation='relu'),\n    BatchNormalization(),\n    Dropout(0.5),\n    \n    UpSampling2D((3, 3)),\n    Conv2DTranspose(64, (5, 5), activation='relu'),\n    BatchNormalization(),\n    Dropout(0.5),\n    \n    Conv2DTranspose(64, (5, 6), activation='relu'),\n    BatchNormalization(),\n    Dropout(0.5),\n    \n    Conv2D(1, (1, 1), activation='sigmoid')\n], name='decoder')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoencoder = keras.Model(inputs, decoder(x), name='vae')\nprint(autoencoder.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoencoder.compile(optimizer='adam', \n                    loss='mean_squared_error', \n                    metrics=['mae']\n                   )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callback = EarlyStopping(monitor='val_loss', \n                         patience=15,\n                         restore_best_weights=True)\n\nhist = autoencoder.fit(X_train, y_train,\n                epochs=10,\n                batch_size=32,\n                shuffle=True,\n                validation_data=(X_val, y_val),\n                callbacks=[callback],\n               )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how loss & mae went down\nepoch_loss = hist.history['loss']\nepoch_val_loss = hist.history['val_loss']\nepoch_mae = hist.history['mae']\nepoch_val_mae = hist.history['val_mae']\n\nplt.figure(figsize=(20,6))\nplt.subplot(1,2,1)\nplt.plot(range(0,len(epoch_loss)), epoch_loss, 'b-', linewidth=2, label='Train Loss')\nplt.plot(range(0,len(epoch_val_loss)), epoch_val_loss, 'r-', linewidth=2, label='Val Loss')\nplt.title('Evolution of loss on train & validation datasets over epochs')\nplt.legend(loc='best')\n\nplt.subplot(1,2,2)\nplt.plot(range(0,len(epoch_mae)), epoch_mae, 'b-', linewidth=2, label='Train MAE')\nplt.plot(range(0,len(epoch_val_mae)), epoch_val_mae, 'r-', linewidth=2,label='Val MAE')\nplt.title('Evolution of MAE on train & validation datasets over epochs')\nplt.legend(loc='best')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\nnote: we need to crop the image -> prediction -> reconstruction","metadata":{}},{"cell_type":"code","source":"def reconstruct_img(img_list, path):\n    # obtain original image and borders dimensions\n    img = cv2.imread(path)\n    img_h, img_w = img.shape[0], img.shape[1]\n    \n    UD_border = img_dim[0] * ((img_h - 1) // img_dim[0] + 1) - img_h\n    U, D = UD_border // 2, math.ceil(UD_border / 2)\n    \n    LR_border = img_dim[1] * ((img_w - 1) // img_dim[1] + 1) - img_w\n    L, R = LR_border // 2, math.ceil(LR_border / 2)\n    \n    no_row, no_col = (img_h + UD_border) // img_dim[0], (img_w + LR_border) // img_dim[1]\n    \n#     print(\"image size\", img.shape)\n#     print(\"paddings\", U, D, L, R)\n    \n    # concatenate the grids back into one image\n    # concat by row first\n    row_imgs = []\n    for row in range(no_row):\n        im_r = cv2.hconcat([img_list[row * no_col + col] for col in range(no_col)])\n#         print(im_r.shape)\n        row_imgs.append(im_r)\n        \n    # concat all rows to form 1 image\n    padded_img = cv2.vconcat(row_imgs)\n            \n    # remove paddings\n    if D > 0 and R > 0:\n        crop_img = padded_img[U:-D, L:-R]\n    elif D > 0:\n        crop_img = padded_img[U:-D]\n    elif R > 0:\n        crop_img = padded_img[:, L:-R]\n    else:\n        crop_img = padded_img\n        \n#     print(padded_img.shape, crop_img.shape)\n    return crop_img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = []\nc = 0\n\nfor f in sorted(os.listdir(path + 'test/')):\n    img_path = path + 'test/' + f\n    \n    # predict\n    processed_img = np.asarray(load_img(img_path))\n    pred_arr = autoencoder.predict(processed_img, batch_size=16, verbose=0)\n    \n    # process the output\n    pred_list = [pred_arr[i] for i in range(pred_arr.shape[0])]\n    processed_output = reconstruct_img(pred_list, img_path)\n    y_test.append(processed_output)\n    \n#     print(processed_output.shape)\n#     c += 1\n#     if c > 4:\n#         break\nprint(len(y_test))\n#     plt.imshow(processed_output, cmap='gray')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# it will take a while!\nids = []\nvals = []\nfor i, f in enumerate(test_img):\n    file = path + 'test/' + f\n    imgid = int(f[:-4])\n    img = cv2.imread(file, 0)\n    img_shape = img.shape\n    # print('Processing image: {} \\tinto size: {}'.format(f, img_shape))    # uncomment to see progress\n    preds_reshaped = cv2.resize(y_test[i], (img_shape[1], img_shape[0]))\n\n    for r in range(img_shape[0]):\n        for c in range(img_shape[1]):\n            ids.append(str(imgid)+'_'+str(r + 1)+'_'+str(c + 1))\n            vals.append(preds_reshaped[r, c])\n\nsubmission = pd.DataFrame({'id': ids, 'value': vals})\nsubmission.to_csv('submission.csv',index = False)\n\nprint('Results saved to submission.csv!')\nprint('Length of IDs: {}'.format(len(ids)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.rmtree(path + 'train/')\nshutil.rmtree(path + 'test/')\nshutil.rmtree(path + 'train_cleaned/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}