{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"230513 23:30\n\n10 epoch - submission(14).csv -> 0.02683\n\n100 epoch - version 3 -> 0.0163\n\ncropping + CNN\n```\nautoencoder = keras.Sequential([\n    layers.Input(shape=img_dim),\n    # encoder\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    \n    layers.Conv2D(256, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    \n    layers.Conv2D(128, (3, 3), activation='relu'),\n    \n    layers.Dropout(0.5),\n\n    # decoder\n    layers.Conv2DTranspose(128, (3, 3), activation='relu'),\n    \n    layers.Conv2DTranspose(256, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    \n    layers.Conv2DTranspose(64, (3, 3), activation='relu'),\n    layers.Conv2D(1, (1, 1), activation='sigmoid')\n])\n```","metadata":{}},{"cell_type":"code","source":"import zipfile\nimport os\nfrom collections import Counter\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport math\n\nimport keras\nfrom keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-13T14:47:59.758211Z","iopub.execute_input":"2023-05-13T14:47:59.759027Z","iopub.status.idle":"2023-05-13T14:48:08.699752Z","shell.execute_reply.started":"2023-05-13T14:47:59.758986Z","shell.execute_reply":"2023-05-13T14:48:08.698774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"markdown","source":"### Unzip images into workspace directories","metadata":{}},{"cell_type":"code","source":"path_zip = '/kaggle/input/denoising-dirty-documents/'\npath = '/kaggle/working/'\n\nwith zipfile.ZipFile(path_zip + 'train.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)\n\nwith zipfile.ZipFile(path_zip + 'test.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)  \n    \nwith zipfile.ZipFile(path_zip + 'train_cleaned.zip', 'r') as zip_ref:\n    zip_ref.extractall(path)  \n    \n# list of image names\ntrain_img = sorted(os.listdir(path + '/train'))\ntrain_cleaned_img = sorted(os.listdir(path + '/train_cleaned'))\ntest_img = sorted(os.listdir(path + '/test'))","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:08.701707Z","iopub.execute_input":"2023-05-13T14:48:08.702528Z","iopub.status.idle":"2023-05-13T14:48:09.610871Z","shell.execute_reply.started":"2023-05-13T14:48:08.702494Z","shell.execute_reply":"2023-05-13T14:48:09.609856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"crop to 140 (height, 420/3) x 135(width,540/4)","metadata":{}},{"cell_type":"markdown","source":"## Preprocess Image","metadata":{}},{"cell_type":"code","source":"img_dim = (140, 135, 1)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:09.614699Z","iopub.execute_input":"2023-05-13T14:48:09.614973Z","iopub.status.idle":"2023-05-13T14:48:09.620983Z","shell.execute_reply.started":"2023-05-13T14:48:09.614948Z","shell.execute_reply":"2023-05-13T14:48:09.620067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(img):\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img_np = np.asarray(img, dtype=\"float32\") \n    img_np /= 255.0\n    img_np = np.reshape(img_np, img_dim) # expand to 3d\n    return img_np","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:09.624232Z","iopub.execute_input":"2023-05-13T14:48:09.625390Z","iopub.status.idle":"2023-05-13T14:48:09.630917Z","shell.execute_reply.started":"2023-05-13T14:48:09.625356Z","shell.execute_reply":"2023-05-13T14:48:09.629903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def crop_image(img):\n    img_h, img_w = img.shape[0], img.shape[1]\n    \n    # center image by adding white paddings\n    UD_border = img_dim[0] * ((img_h - 1) // img_dim[0] + 1) - img_h\n    U, D = UD_border // 2, math.ceil(UD_border / 2)\n    \n    LR_border = img_dim[1] * ((img_w - 1) // img_dim[1] + 1) - img_w\n    L, R = LR_border // 2, math.ceil(LR_border / 2)\n    \n    padded_img = cv2.copyMakeBorder(img, U, D, L, R, cv2.BORDER_CONSTANT, value=(255, 255, 255))\n    \n    # crop image\n    img_list = []\n    no_row, no_col = (img_h + UD_border) // img_dim[0], (img_w + LR_border) // img_dim[1]\n    for row in range(no_row):\n        for col in range(no_col):\n            r1, r2 = row * img_dim[0], (row + 1) * img_dim[0]\n            c1, c2 = col * img_dim[1], (col + 1) * img_dim[1]\n            crop_img = padded_img[r1:r2, c1:c2]\n            img_list.append(crop_img)\n    \n    return img_list","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:09.632516Z","iopub.execute_input":"2023-05-13T14:48:09.632888Z","iopub.status.idle":"2023-05-13T14:48:09.643573Z","shell.execute_reply.started":"2023-05-13T14:48:09.632856Z","shell.execute_reply":"2023-05-13T14:48:09.642725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_img(path):\n    # load image\n    raw_img = cv2.imread(path)\n    \n    # crop image\n    img_list = crop_image(raw_img)\n    \n    # preprocess - to gray, normalize, to 3d\n    processed_list = []\n    for img in img_list:\n        processed_img = preprocess_image(img)\n        processed_list.append(processed_img)\n        \n    return processed_list","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:09.645202Z","iopub.execute_input":"2023-05-13T14:48:09.646043Z","iopub.status.idle":"2023-05-13T14:48:09.653515Z","shell.execute_reply.started":"2023-05-13T14:48:09.646012Z","shell.execute_reply":"2023-05-13T14:48:09.652581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_imgs(img_list):\n    n = len(img_list)\n    for i in range(n):\n        plt.subplot(math.ceil(n/4),4,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.imshow(img_list[i], cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:09.656966Z","iopub.execute_input":"2023-05-13T14:48:09.657248Z","iopub.status.idle":"2023-05-13T14:48:09.666493Z","shell.execute_reply.started":"2023-05-13T14:48:09.657224Z","shell.execute_reply":"2023-05-13T14:48:09.665553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess images\ntrain = []\ntrain_cleaned = []\ntest = []\n\nfor f in sorted(os.listdir(path + 'train/')):\n    processed_img = load_img(path + 'train/' + f)\n    for img in processed_img:\n        train.append(img)\n\nfor f in sorted(os.listdir(path + 'train_cleaned/')):\n    processed_img = load_img(path + 'train_cleaned/' + f)\n    for img in processed_img:\n        train_cleaned.append(img)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:09.667823Z","iopub.execute_input":"2023-05-13T14:48:09.668155Z","iopub.status.idle":"2023-05-13T14:48:10.641751Z","shell.execute_reply.started":"2023-05-13T14:48:09.668123Z","shell.execute_reply":"2023-05-13T14:48:10.640683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_imgs(train[4:8])","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:10.643632Z","iopub.execute_input":"2023-05-13T14:48:10.644024Z","iopub.status.idle":"2023-05-13T14:48:10.904649Z","shell.execute_reply.started":"2023-05-13T14:48:10.643987Z","shell.execute_reply":"2023-05-13T14:48:10.903178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_imgs(train_cleaned[4:8])","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:10.908821Z","iopub.execute_input":"2023-05-13T14:48:10.909319Z","iopub.status.idle":"2023-05-13T14:48:11.125015Z","shell.execute_reply.started":"2023-05-13T14:48:10.909266Z","shell.execute_reply":"2023-05-13T14:48:11.124137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train test split\nThe data size is small, so use a smaller portion of validation set","metadata":{}},{"cell_type":"code","source":"X_train = np.asarray(train)\ny_train = np.asarray(train_cleaned)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:11.126579Z","iopub.execute_input":"2023-05-13T14:48:11.127222Z","iopub.status.idle":"2023-05-13T14:48:11.301277Z","shell.execute_reply.started":"2023-05-13T14:48:11.127188Z","shell.execute_reply":"2023-05-13T14:48:11.300304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:11.302870Z","iopub.execute_input":"2023-05-13T14:48:11.303229Z","iopub.status.idle":"2023-05-13T14:48:11.311629Z","shell.execute_reply.started":"2023-05-13T14:48:11.303195Z","shell.execute_reply":"2023-05-13T14:48:11.308665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Autoencoder","metadata":{}},{"cell_type":"code","source":"autoencoder = keras.Sequential([\n    layers.Input(shape=img_dim),\n    # encoder\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    \n    layers.Conv2D(256, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    \n    layers.Conv2D(128, (3, 3), activation='relu'),\n    \n    layers.Dropout(0.5),\n\n    # decoder\n    layers.Conv2DTranspose(128, (3, 3), activation='relu'),\n    \n    layers.Conv2DTranspose(256, (3, 3), activation='relu'),\n    layers.BatchNormalization(),\n    \n    layers.Conv2DTranspose(64, (3, 3), activation='relu'),\n    layers.Conv2D(1, (1, 1), activation='sigmoid')\n])","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:11.313491Z","iopub.execute_input":"2023-05-13T14:48:11.313976Z","iopub.status.idle":"2023-05-13T14:48:14.043746Z","shell.execute_reply.started":"2023-05-13T14:48:11.313943Z","shell.execute_reply":"2023-05-13T14:48:14.042818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(autoencoder.summary())","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:14.045142Z","iopub.execute_input":"2023-05-13T14:48:14.045518Z","iopub.status.idle":"2023-05-13T14:48:14.081412Z","shell.execute_reply.started":"2023-05-13T14:48:14.045486Z","shell.execute_reply":"2023-05-13T14:48:14.080686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoencoder.compile(optimizer='adam', \n                    loss='mean_squared_error', \n                    metrics=['mae']\n                   )","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:14.082596Z","iopub.execute_input":"2023-05-13T14:48:14.082933Z","iopub.status.idle":"2023-05-13T14:48:14.116943Z","shell.execute_reply.started":"2023-05-13T14:48:14.082902Z","shell.execute_reply":"2023-05-13T14:48:14.115988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callback = EarlyStopping(monitor='val_loss', patience=15)\n\nhist = autoencoder.fit(X_train, y_train,\n                epochs=300,\n                batch_size=16,\n                shuffle=True,\n                validation_data=(X_val, y_val),\n                callbacks=[callback],\n               )","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:48:14.118573Z","iopub.execute_input":"2023-05-13T14:48:14.119228Z","iopub.status.idle":"2023-05-13T14:51:38.220099Z","shell.execute_reply.started":"2023-05-13T14:48:14.119193Z","shell.execute_reply":"2023-05-13T14:51:38.219124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how loss & mae went down\nepoch_loss = hist.history['loss']\nepoch_val_loss = hist.history['val_loss']\nepoch_mae = hist.history['mae']\nepoch_val_mae = hist.history['val_mae']\n\nplt.figure(figsize=(20,6))\nplt.subplot(1,2,1)\nplt.plot(range(0,len(epoch_loss)), epoch_loss, 'b-', linewidth=2, label='Train Loss')\nplt.plot(range(0,len(epoch_val_loss)), epoch_val_loss, 'r-', linewidth=2, label='Val Loss')\nplt.title('Evolution of loss on train & validation datasets over epochs')\nplt.legend(loc='best')\n\nplt.subplot(1,2,2)\nplt.plot(range(0,len(epoch_mae)), epoch_mae, 'b-', linewidth=2, label='Train MAE')\nplt.plot(range(0,len(epoch_val_mae)), epoch_val_mae, 'r-', linewidth=2,label='Val MAE')\nplt.title('Evolution of MAE on train & validation datasets over epochs')\nplt.legend(loc='best')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-13T14:51:38.223660Z","iopub.execute_input":"2023-05-13T14:51:38.224191Z","iopub.status.idle":"2023-05-13T14:51:38.662174Z","shell.execute_reply.started":"2023-05-13T14:51:38.224141Z","shell.execute_reply":"2023-05-13T14:51:38.661110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\nnote: we need to crop the image -> prediction -> reconstruction","metadata":{}},{"cell_type":"code","source":"def reconstruct_img(img_list, path):\n    # obtain original image and borders dimensions\n    img = cv2.imread(path)\n    img_h, img_w = img.shape[0], img.shape[1]\n    \n    UD_border = img_dim[0] * ((img_h - 1) // img_dim[0] + 1) - img_h\n    U, D = UD_border // 2, math.ceil(UD_border / 2)\n    \n    LR_border = img_dim[1] * ((img_w - 1) // img_dim[1] + 1) - img_w\n    L, R = LR_border // 2, math.ceil(LR_border / 2)\n    \n    no_row, no_col = (img_h + UD_border) // img_dim[0], (img_w + LR_border) // img_dim[1]\n    \n#     print(\"image size\", img.shape)\n#     print(\"paddings\", U, D, L, R)\n    \n    # concatenate the grids back into one image\n    # concat by row first\n    row_imgs = []\n    for row in range(no_row):\n        im_r = cv2.hconcat([img_list[row * no_col + col] for col in range(no_col)])\n#         print(im_r.shape)\n        row_imgs.append(im_r)\n        \n    # concat all rows to form 1 image\n    padded_img = cv2.vconcat(row_imgs)\n            \n    # remove paddings\n    if D > 0 and R > 0:\n        crop_img = padded_img[U:-D, L:-R]\n    elif D > 0:\n        crop_img = padded_img[U:-D]\n    elif R > 0:\n        crop_img = padded_img[:, L:-R]\n    else:\n        crop_img = padded_img\n        \n#     print(padded_img.shape, crop_img.shape)\n    return crop_img","metadata":{"execution":{"iopub.status.busy":"2023-05-13T15:19:50.276781Z","iopub.execute_input":"2023-05-13T15:19:50.278456Z","iopub.status.idle":"2023-05-13T15:19:50.290055Z","shell.execute_reply.started":"2023-05-13T15:19:50.278414Z","shell.execute_reply":"2023-05-13T15:19:50.288985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = []\nc = 0\n\nfor f in sorted(os.listdir(path + 'test/')):\n    img_path = path + 'test/' + f\n    \n    # predict\n    processed_img = np.asarray(load_img(img_path))\n    pred_arr = autoencoder.predict(processed_img, batch_size=16, verbose=0)\n    \n    # process the output\n    pred_list = [pred_arr[i] for i in range(pred_arr.shape[0])]\n    processed_output = reconstruct_img(pred_list, img_path)\n    y_test.append(processed_output)\n    \n#     print(processed_output.shape)\n#     c += 1\n#     if c > 4:\n#         break\nprint(len(y_test))\n#     plt.imshow(processed_output, cmap='gray')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-13T15:19:50.291281Z","iopub.execute_input":"2023-05-13T15:19:50.291616Z","iopub.status.idle":"2023-05-13T15:19:58.246251Z","shell.execute_reply.started":"2023-05-13T15:19:50.291588Z","shell.execute_reply":"2023-05-13T15:19:58.245232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# it will take a while!\nids = []\nvals = []\nfor i, f in enumerate(test_img):\n    file = path + 'test/' + f\n    imgid = int(f[:-4])\n    img = cv2.imread(file, 0)\n    img_shape = img.shape\n    # print('Processing image: {} \\tinto size: {}'.format(f, img_shape))    # uncomment to see progress\n    preds_reshaped = cv2.resize(y_test[i], (img_shape[1], img_shape[0]))\n\n    for r in range(img_shape[0]):\n        for c in range(img_shape[1]):\n            ids.append(str(imgid)+'_'+str(r + 1)+'_'+str(c + 1))\n            vals.append(preds_reshaped[r, c])\n\nsubmission = pd.DataFrame({'id': ids, 'value': vals})\nsubmission.to_csv('submission.csv',index = False)\n\nprint('Results saved to submission.csv!')\nprint('Length of IDs: {}'.format(len(ids)))","metadata":{"execution":{"iopub.status.busy":"2023-05-13T15:20:02.883898Z","iopub.execute_input":"2023-05-13T15:20:02.884291Z","iopub.status.idle":"2023-05-13T15:21:12.794379Z","shell.execute_reply.started":"2023-05-13T15:20:02.884260Z","shell.execute_reply":"2023-05-13T15:21:12.793366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.rmtree(path + 'train/')\nshutil.rmtree(path + 'test/')\nshutil.rmtree(path + 'train_cleaned/')","metadata":{"execution":{"iopub.status.busy":"2023-05-13T15:21:12.796427Z","iopub.execute_input":"2023-05-13T15:21:12.796770Z","iopub.status.idle":"2023-05-13T15:21:12.818148Z","shell.execute_reply.started":"2023-05-13T15:21:12.796736Z","shell.execute_reply":"2023-05-13T15:21:12.817259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-05-13T15:21:12.819547Z","iopub.execute_input":"2023-05-13T15:21:12.820326Z","iopub.status.idle":"2023-05-13T15:21:20.631486Z","shell.execute_reply.started":"2023-05-13T15:21:12.820288Z","shell.execute_reply":"2023-05-13T15:21:20.630397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}